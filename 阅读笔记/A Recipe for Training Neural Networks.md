# A Recipe for Training Neural Networks

### 1. 开始训练之前，先对数据了熟于心

例如数据集的背景是什么？有多少变化，形式是什么？什么变化是假的，可以预处理？

### 2. 设置端到端的评估框架

这一步应该建立一个完整的训练模型+评估框架，并测试其正确性。最好选择一种有把握的简单模型，例如线性分类器或非常小的ConvNet。

**Tips:**

**固定随机种子**

使用固定的随机种子来确保代码每次运行的结果都相同。

**简化**

不要加入过多数据。

**验证损失函数**

用正确的损失值来验证损失函数。

**恰当的初始化**

正确初始化最后一层的权重。当数据集不平衡时，例如positives:negatives=1:10，设置好偏差以便网络的预测概率初始化时为0.1.

**过拟合少量数据**

这样之后我们可以增加模型的容量（例如增加layer等），就可以知道我们能够达到的最低loss的值。

**可视化**

在训练之前首先可视化网络中的内容，可以揭示出很多问题。除此之外，在训练过程中可以对固定的测试数据的预测结果可视化。

### 3. Overfit

**Tips:**

**选择模型**

根据数据选择合适的网络结构，需要注意的是，在项目初期应该从简单的模型开始，之后再逐渐调整。

**Adam优化器是靠谱的**

**提高模型的复杂程度时，每次只改动一处**

**不要相信默认的学习速率衰减率**

对于不同的问题，应该使用不同的学习速率衰减方法。而且对于特定的问题，衰减速率应当基于当前的epoch数。

### 4. Regularize（正则化）

现在已经有了一个有效的模型，接下来应该放弃一些训练精度，提高验证集上的精度。

**Tips:**

**获取更多数据**

除了收集更多数据之外，还可以加入半真半假的数据，或者人为地创造数据。

**预训练**

即便你有足够的数据，如果可以的话，还是建议使用经过预先训练的网络。

**坚持有监督学习**

不要太执着于无监督的预先训练。

**减少输入维度**

删除可能包含虚假信息的输入。

**缩减模型大小**

**减小batch大小**

由于batch内部的规范化，较小的batch在一定程度上对应着较强的规范化。

**防止过拟合**

添加dropout，但是dropout似乎不能很好地处理批处理规范化。

**权重衰减**

增加权重衰减惩罚力度。

**及时停止训练**

基于验证集上的loss，及时停止训练，防止过拟合。



最后，可以可视化网络的第一层权重，确保结果有意义。此外，网络内的激活函数有时也会出现奇怪的效应。

### 5. 调参

随机网格搜索、超级参数优化

### 6. 进一步提高模型效果

**模型集合/合并**

非常靠谱的方法，很可能会提高2%的精度。

**持续训练**

有时，模型可以长时间不间断地训练并不断优化提升。